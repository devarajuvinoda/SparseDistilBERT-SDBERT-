{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sparse_attention_KT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N94UyOdA0mCO",
        "outputId": "e8eea2f1-28c4-498a-cd7b-11517cb44771"
      },
      "source": [
        "!pip install git+https://github.com/google-research/bigbird.git -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 33.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 35.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 31.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 367 kB 43.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 48.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 191 kB 48.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 27.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 366 kB 30.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 251 kB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 191 kB 50.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 52.0 MB/s \n",
            "\u001b[?25h  Building wheel for bigbird (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0irPwcbBYvDV"
      },
      "source": [
        "from bigbird.core import flags\n",
        "from bigbird.core import modeling\n",
        "from bigbird.core import utils\n",
        "from bigbird.classifier import run_classifier\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "if not hasattr(FLAGS, \"f\"): flags.DEFINE_string(\"f\", \"\", \"\")\n",
        "FLAGS(sys.argv)\n",
        "\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rph2sJ75kBNA"
      },
      "source": [
        "FLAGS.data_dir = \"tfds://imdb_reviews/plain_text\"\n",
        "FLAGS.attention_type = \"block_sparse\"\n",
        "FLAGS.max_encoder_length = 512  # reduce for quicker demo on free colab\n",
        "FLAGS.learning_rate = 1e-5\n",
        "FLAGS.num_train_steps = 1000\n",
        "FLAGS.attention_probs_dropout_prob = 0.0\n",
        "FLAGS.hidden_dropout_prob = 0.0\n",
        "FLAGS.use_gradient_checkpointing = True\n",
        "FLAGS.vocab_model_file = \"gpt2\"\n",
        "FLAGS.do_export = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuxI3V_3j57Y"
      },
      "source": [
        "bert_config = flags.as_dictionary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7HoSGZG154T"
      },
      "source": [
        "# bert_config['intermediate_size'] = 2048\n",
        "# bert_config['hidden_size']\n",
        "# bert_config['iterations_per_loop']= '500'\n",
        "# bert_config['num_attention_heads']= 8\n",
        "# bert_config['num_hidden_layers'] = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDKMYcc42hB0",
        "outputId": "8390e5f1-3108-4190-ad33-32f129d7d51d"
      },
      "source": [
        "bert_config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.0,\n",
              " 'attention_type': 'block_sparse',\n",
              " 'block_size': 16,\n",
              " 'data_dir': 'tfds://imdb_reviews/plain_text',\n",
              " 'do_eval': False,\n",
              " 'do_export': True,\n",
              " 'do_train': True,\n",
              " 'eval_batch_size': 8,\n",
              " 'gcp_project': None,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.0,\n",
              " 'hidden_size': 768,\n",
              " 'init_checkpoint': None,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'iterations_per_loop': '1000',\n",
              " 'learning_rate': 1e-05,\n",
              " 'master': None,\n",
              " 'max_encoder_length': 512,\n",
              " 'max_position_embeddings': 4096,\n",
              " 'norm_type': 'postnorm',\n",
              " 'num_attention_heads': 12,\n",
              " 'num_hidden_layers': 12,\n",
              " 'num_labels': 2,\n",
              " 'num_rand_blocks': 3,\n",
              " 'num_tpu_cores': 8,\n",
              " 'num_train_steps': 1000,\n",
              " 'num_warmup_steps': 1000,\n",
              " 'optimizer': 'AdamWeightDecay',\n",
              " 'optimizer_beta1': 0.9,\n",
              " 'optimizer_beta2': 0.999,\n",
              " 'optimizer_epsilon': 1e-06,\n",
              " 'output_dir': '/tmp/bigb',\n",
              " 'rescale_embedding': False,\n",
              " 'save_checkpoints_steps': 1000,\n",
              " 'scope': 'bert',\n",
              " 'substitute_newline': None,\n",
              " 'tpu_job_name': None,\n",
              " 'tpu_name': None,\n",
              " 'tpu_zone': None,\n",
              " 'train_batch_size': 8,\n",
              " 'type_vocab_size': 2,\n",
              " 'use_bias': True,\n",
              " 'use_gradient_checkpointing': True,\n",
              " 'use_tpu': False,\n",
              " 'vocab_model_file': '/usr/local/lib/python3.7/dist-packages/bigbird/vocab/gpt2.model',\n",
              " 'vocab_size': 50358,\n",
              " 'weight_decay_rate': 0.01}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRF4TUEQxjXJ"
      },
      "source": [
        "## Define classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3yNdo5toQwq"
      },
      "source": [
        "model = modeling.BertModel(bert_config)\n",
        "headl = run_classifier.ClassifierLossLayer(\n",
        "        bert_config[\"hidden_size\"], bert_config[\"num_labels\"],\n",
        "        bert_config[\"hidden_dropout_prob\"],\n",
        "        utils.create_initializer(bert_config[\"initializer_range\"]),\n",
        "        name=bert_config[\"scope\"]+\"/classifier\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXOY78vbqHX9"
      },
      "source": [
        "@tf.function(experimental_compile=True)\n",
        "def fwd_bwd(features, labels):\n",
        "  with tf.GradientTape() as g:\n",
        "    _, pooled_output = model(features, training=True)\n",
        "    loss, log_probs = headl(pooled_output, labels, True)\n",
        "    # print(\"loss:\",loss)\n",
        "  grads = g.gradient(loss, model.trainable_weights+headl.trainable_weights)\n",
        "  return loss, log_probs, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzoTMyQlxsRo"
      },
      "source": [
        "## Dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo-NQTDZZs51"
      },
      "source": [
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=True)\n",
        "dataset = train_input_fn({'batch_size': 32})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwykF232o2uC"
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWjkDvu9k7ie",
        "outputId": "e1d75b84-33ac-4378-e61d-aafc7d9aabd0"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(FLAGS.learning_rate)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "for i, ex in enumerate(tqdm(dataset.take(FLAGS.num_train_steps), position=0)):\n",
        "  loss, log_probs, grads = fwd_bwd(ex[0], ex[1])\n",
        "  opt.apply_gradients(zip(grads, model.trainable_weights+headl.trainable_weights))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(tf.one_hot(ex[1], 2), log_probs)\n",
        "  if (i+1)% 50 == 0:\n",
        "    print('Loss = {}  Accuracy = {}'.format(train_loss.result().numpy(), train_accuracy.result().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 5%|▌         | 50/1000 [01:06<15:43,  1.01it/s]Loss = 0.69440758228302  Accuracy = 0.4975000023841858\n",
            " 10%|█         | 100/1000 [01:56<14:54,  1.01it/s]Loss = 0.6949686408042908  Accuracy = 0.4987500011920929\n",
            " 15%|█▌        | 150/1000 [02:45<14:01,  1.01it/s]Loss = 0.6937657594680786  Accuracy = 0.5091666579246521\n",
            " 20%|██        | 200/1000 [03:35<13:12,  1.01it/s]Loss = 0.6920759081840515  Accuracy = 0.5221874713897705\n",
            " 25%|██▌       | 250/1000 [04:24<12:28,  1.00it/s]Loss = 0.6906613111495972  Accuracy = 0.5267500281333923\n",
            " 30%|███       | 300/1000 [05:14<11:35,  1.01it/s]Loss = 0.6866089105606079  Accuracy = 0.5407291650772095\n",
            " 35%|███▌      | 350/1000 [06:03<10:48,  1.00it/s]Loss = 0.6828024387359619  Accuracy = 0.5516071319580078\n",
            " 40%|████      | 400/1000 [06:53<09:57,  1.00it/s]Loss = 0.6743711829185486  Accuracy = 0.567578136920929\n",
            " 45%|████▌     | 450/1000 [07:43<09:03,  1.01it/s]Loss = 0.6637136340141296  Accuracy = 0.614583301544109\n",
            " 50%|█████     | 500/1000 [08:32<08:13,  1.01it/s]Loss = 0.6512491703033447  Accuracy = 0.6395687508583068\n",
            " 55%|█████▌    | 550/1000 [09:21<07:25,  1.01it/s]Loss = 0.6389033198356628  Accuracy = 0.696590757369995\n",
            " 60%|██████    | 600/1000 [10:11<06:34,  1.01it/s]Loss = 0.6272562742233276  Accuracy = 0.7217708587646484\n",
            " 65%|██████▌   | 650/1000 [11:00<05:45,  1.01it/s]Loss = 0.614691972732544  Accuracy = 0.7338461637496948\n",
            " 70%|███████   | 700/1000 [11:49<04:57,  1.01it/s]Loss = 0.6039265990257263  Accuracy = 0.7443750262260437\n",
            " 75%|███████▌  | 750/1000 [12:39<04:07,  1.01it/s]Loss = 0.5932496190071106  Accuracy = 0.83534166932106018\n",
            " 80%|████████  | 800/1000 [13:28<03:18,  1.01it/s]Loss = 0.5813524723052979  Accuracy = 0.8637109518051147\n",
            " 85%|████████▌ | 850/1000 [14:18<02:28,  1.01it/s]Loss = 0.5708072185516357  Accuracy = 0.8727573275566101\n",
            " 90%|█████████ | 900/1000 [15:07<01:39,  1.01it/s]Loss = 0.5599746108055115  Accuracy = 0.8814236044883728\n",
            " 95%|█████████▌| 950/1000 [15:57<00:49,  1.02it/s]Loss = 0.5526861548423767  Accuracy = 0.907598705291748\n",
            "100%|██████████| 1000/1000 [16:46<00:00,  1.01s/it]Loss = 0.543519139289856  Accuracy = 0.9150312256813049\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "012XZP3MpVw8",
        "outputId": "9987b29a-0068-485a-f2f5-c77196ff76ae"
      },
      "source": [
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 6935.456838846207 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc3OatxlMCk3"
      },
      "source": [
        "@tf.function(experimental_compile=True)\n",
        "def fwd_only(features, labels):\n",
        "  _, pooled_output = model(features, training=False)\n",
        "  loss, log_probs = headl(pooled_output, labels, False)\n",
        "  return loss, log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mq_xhMzef42"
      },
      "source": [
        "eval_input_fn = run_classifier.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=False)\n",
        "eval_dataset = eval_input_fn({'batch_size': 32})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqPN4R8kerUG",
        "outputId": "3bb20f7d-6c63-4637-ad6e-1a4d6fa27b15"
      },
      "source": [
        "eval_loss = tf.keras.metrics.Mean(name='eval_loss')\n",
        "eval_accuracy = tf.keras.metrics.CategoricalAccuracy(name='eval_accuracy')\n",
        "\n",
        "for i, ex in enumerate(tqdm(eval_dataset, position=0)):\n",
        "  loss, log_probs = fwd_only(ex[0], ex[1])\n",
        "  eval_loss(loss)\n",
        "  eval_accuracy(tf.one_hot(ex[1], 2), log_probs)\n",
        "\n",
        "print('Loss = {}  Accuracy = {}'.format(eval_loss.result().numpy(), eval_accuracy.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 781/781 [03:11<00:00,  4.08it/s]Loss = 0.378199964761734  Accuracy = 0.90625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlISwSH85BWm"
      },
      "source": [
        "student_bert_config = flags.as_dictionary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F4t-vzN5BUC"
      },
      "source": [
        "student_bert_config['intermediate_size'] = 512\n",
        "# student_bert_config['hidden_size']= 256\n",
        "# student_bert_config['iterations_per_loop']= '800'\n",
        "student_bert_config['num_attention_heads']= 4 # 12 for teacher model\n",
        "student_bert_config['num_hidden_layers'] = 3 # 12 for teacher model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBtDaUy5BRh",
        "outputId": "cd75d4fb-86cb-4256-eec7-47a98ff470e3"
      },
      "source": [
        "student_bert_config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.0,\n",
              " 'attention_type': 'block_sparse',\n",
              " 'block_size': 16,\n",
              " 'data_dir': 'tfds://imdb_reviews/plain_text',\n",
              " 'do_eval': False,\n",
              " 'do_export': True,\n",
              " 'do_train': True,\n",
              " 'eval_batch_size': 8,\n",
              " 'gcp_project': None,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.0,\n",
              " 'hidden_size': 256,\n",
              " 'init_checkpoint': None,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 512,\n",
              " 'iterations_per_loop': '1000',\n",
              " 'learning_rate': 1e-05,\n",
              " 'master': None,\n",
              " 'max_encoder_length': 512,\n",
              " 'max_position_embeddings': 4096,\n",
              " 'norm_type': 'postnorm',\n",
              " 'num_attention_heads': 4,\n",
              " 'num_hidden_layers': 3,\n",
              " 'num_labels': 2,\n",
              " 'num_rand_blocks': 3,\n",
              " 'num_tpu_cores': 8,\n",
              " 'num_train_steps': 1000,\n",
              " 'num_warmup_steps': 1000,\n",
              " 'optimizer': 'AdamWeightDecay',\n",
              " 'optimizer_beta1': 0.9,\n",
              " 'optimizer_beta2': 0.999,\n",
              " 'optimizer_epsilon': 1e-06,\n",
              " 'output_dir': '/tmp/bigb',\n",
              " 'rescale_embedding': False,\n",
              " 'save_checkpoints_steps': 1000,\n",
              " 'scope': 'bert',\n",
              " 'substitute_newline': None,\n",
              " 'tpu_job_name': None,\n",
              " 'tpu_name': None,\n",
              " 'tpu_zone': None,\n",
              " 'train_batch_size': 8,\n",
              " 'type_vocab_size': 2,\n",
              " 'use_bias': True,\n",
              " 'use_gradient_checkpointing': True,\n",
              " 'use_tpu': False,\n",
              " 'vocab_model_file': '/usr/local/lib/python3.7/dist-packages/bigbird/vocab/gpt2.model',\n",
              " 'vocab_size': 50358,\n",
              " 'weight_decay_rate': 0.01}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OWLgYvS5WK5",
        "outputId": "1c518fe1-f3e1-45bf-9cb7-ecf6f1e3eeef"
      },
      "source": [
        "import numpy as np\n",
        "alpha = 0.2\n",
        "\n",
        "student_model = modeling.BertModel(student_bert_config)\n",
        "student_headl = run_classifier.ClassifierLossLayer(\n",
        "        student_bert_config[\"hidden_size\"], student_bert_config[\"num_labels\"],\n",
        "        student_bert_config[\"hidden_dropout_prob\"],\n",
        "        utils.create_initializer(student_bert_config[\"initializer_range\"]),\n",
        "        name=student_bert_config[\"scope\"]+\"/classifier\")\n",
        "        \n",
        "@tf.function(experimental_compile=True)\n",
        "def student_fwd_bwd(features, labels):\n",
        "  with tf.GradientTape() as g:\n",
        "    _, pooled_output_student = student_model(features, training=True)\n",
        "    student_loss, log_probs_student = student_headl(pooled_output_student, labels, True)\n",
        "    # print(\"log_probs_student: \",log_probs_student)\n",
        "    # one_hot_labels = tf.one_hot(labels, depth=student_bert_config[\"num_labels\"],dtype=tf.float32)\n",
        "    # student_loss = -tf.reduce_sum(one_hot_labels * log_probs_student, axis=-1)\n",
        "    # student_loss = - one_hot_labels * tf.math.log(log_probs_student) - (1 - one_hot_labels) * tf.math.log(1 - log_probs_student)\n",
        "\n",
        "    _, pooled_output_teacher = model(features, training=False)\n",
        "    teacher_loss, log_probs_teacher = headl(pooled_output_teacher, labels, False)\n",
        "    # print(\"log_probs_teacher: \",log_probs_teacher)\n",
        "    \n",
        "    # distil_loss = - log_probs_teacher * tf.math.log(log_probs_student) - (1 - log_probs_teacher) * tf.math.log(1 - log_probs_student)\n",
        "    \n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    distil_loss = mse(log_probs_teacher, log_probs_student)\n",
        "    # print(\"distil_loss.numpy(): \",distil_loss.numpy())\n",
        "    # print(\"student_loss: \",student_loss.shape)\n",
        "    # print(\"distil_loss: \",distil_loss.shape)\n",
        "    # distil_loss = (student_loss-teacher_loss)**2\n",
        "    loss = alpha * student_loss + (1-alpha) * distil_loss\n",
        "    # loss = student_loss\n",
        "    # print('\\nloss:',loss,'\\nstudent_loss:',student_loss)\n",
        "    # loss = tf.reduce_mean(loss, axis=-1)\n",
        "    # tf.print(loss)\n",
        "  grads = g.gradient(loss, student_model.trainable_weights+headl.trainable_weights)\n",
        "  return loss, log_probs_student, grads\n",
        "\n",
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=True)\n",
        "dataset = train_input_fn({'batch_size': 32})\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "opt = tf.keras.optimizers.Adam(FLAGS.learning_rate)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "for i, ex in enumerate(tqdm(dataset.take(FLAGS.num_train_steps), position=0)):\n",
        "  loss, log_probs, grads = student_fwd_bwd(ex[0], ex[1])\n",
        "  opt.apply_gradients(zip(grads, student_model.trainable_weights+student_headl.trainable_weights))\n",
        "  train_loss(loss)\n",
        "  # print(train_loss(loss))\n",
        "  train_accuracy(tf.one_hot(ex[1], 2), log_probs)\n",
        "  if (i+1)% 50 == 0:\n",
        "    print('Loss = {}  Accuracy = {}'.format(train_loss.result().numpy(), train_accuracy.result().numpy()))\n",
        "  # break\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"student_model params:\",student_model.count_params())\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def fwd_only(features, labels):\n",
        "  _, pooled_output = student_model(features, training=False)\n",
        "  loss, log_probs = student_headl(pooled_output, labels, False)\n",
        "  return loss, log_probs\n",
        "\n",
        "\n",
        "eval_input_fn = run_classifier.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=False)\n",
        "eval_dataset = eval_input_fn({'batch_size': 32})\n",
        "\n",
        "eval_loss = tf.keras.metrics.Mean(name='eval_loss')\n",
        "eval_accuracy = tf.keras.metrics.CategoricalAccuracy(name='eval_accuracy')\n",
        "\n",
        "for ex in tqdm(eval_dataset, position=0):\n",
        "  loss, log_probs = fwd_only(ex[0], ex[1])\n",
        "  eval_loss(loss)\n",
        "  eval_accuracy(tf.one_hot(ex[1], 2), log_probs)\n",
        "print('Loss = {}  Accuracy = {}'.format(eval_loss.result().numpy(), eval_accuracy.result().numpy()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "  5%|▌         | 50/1000 [02:53<43:52,  2.77s/it]Loss = 0.43946707248687744  Accuracy = 0.4987500011920929\n",
            " 10%|█         | 100/1000 [05:12<41:36,  2.77s/it]Loss = 0.43933141231536865  Accuracy = 0.49781250953674316\n",
            " 15%|█▌        | 150/1000 [07:30<39:21,  2.78s/it]Loss = 0.4384157359600067  Accuracy = 0.5087500214576721\n",
            " 20%|██        | 200/1000 [09:49<37:00,  2.78s/it]Loss = 0.44259029626846313  Accuracy = 0.5223437547683716\n",
            " 25%|██▌       | 250/1000 [12:07<34:19,  2.75s/it]Loss = 0.4414173364639282  Accuracy = 0.5237500071525574\n",
            " 30%|███       | 300/1000 [14:24<32:06,  2.75s/it]Loss = 0.4424702525138855  Accuracy = 0.5390625\n",
            " 35%|███▌      | 350/1000 [16:43<30:01,  2.77s/it]Loss = 0.4390639066696167  Accuracy = 0.5479464530944824\n",
            " 40%|████      | 400/1000 [19:01<27:46,  2.78s/it]Loss = 0.4311911463737488  Accuracy = 0.5603125095367432\n",
            " 45%|████▌     | 450/1000 [21:20<25:28,  2.78s/it]Loss = 0.4258018434047699  Accuracy = 0.6095139169692993\n",
            " 50%|█████     | 500/1000 [23:39<23:07,  2.77s/it]Loss = 0.4167731702327728  Accuracy = 0.6831249952316284\n",
            " 55%|█████▌    | 550/1000 [25:58<20:47,  2.77s/it]Loss = 0.4075758457183838  Accuracy = 0.6969886183738708\n",
            " 60%|██████    | 600/1000 [28:15<18:18,  2.75s/it]Loss = 0.3963661789894104  Accuracy = 0.7101041436195374\n",
            " 65%|██████▌   | 650/1000 [30:33<16:11,  2.78s/it]Loss = 0.38260477781295776  Accuracy = 0.724471127986908\n",
            " 70%|███████   | 700/1000 [32:52<13:52,  2.78s/it]Loss = 0.3696557283401489  Accuracy = 0.760714435577393\n",
            " 75%|███████▌  | 750/1000 [35:11<11:33,  2.78s/it]Loss = 0.3569420278072357  Accuracy = 0.8474999785423279\n",
            " 80%|████████  | 800/1000 [37:29<09:15,  2.78s/it]Loss = 0.3441997468471527  Accuracy = 0.8587890386581421\n",
            " 85%|████████▌ | 850/1000 [39:48<06:55,  2.77s/it]Loss = 0.3324805796146393  Accuracy = 0.869301450252533\n",
            " 90%|█████████ | 900/1000 [42:07<04:37,  2.77s/it]Loss = 0.32140111923217773  Accuracy = 0.8796180605888367\n",
            " 95%|█████████▌| 950/1000 [44:25<02:18,  2.77s/it]Loss = 0.31447988748550415  Accuracy = 0.8861842274665833\n",
            "100%|██████████| 1000/1000 [46:44<00:00,  2.80s/it]\n",
            "Loss = 0.30588796734809875  Accuracy = 0.8937812566757202\n",
            "--- 2804.711443901062 seconds ---\n",
            "100%|██████████| 781/781 [03:30<00:00,  3.71it/s]Loss = 0.34927695989608765  Accuracy = 0.8794718074798584\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE3o3H_IvH2o",
        "scrolled": false,
        "outputId": "77fa515c-287c-4f09-f5c3-676c8054dc52"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "student_model = modeling.BertModel(student_bert_config)\n",
        "student_headl = run_classifier.ClassifierLossLayer(\n",
        "      student_bert_config[\"hidden_size\"], student_bert_config[\"num_labels\"],\n",
        "      student_bert_config[\"hidden_dropout_prob\"],\n",
        "      utils.create_initializer(student_bert_config[\"initializer_range\"]),\n",
        "      name=student_bert_config[\"scope\"]+\"/classifier\")\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def student_fwd_bwd(features, labels):\n",
        "  with tf.GradientTape() as g:\n",
        "    _, pooled_output_student = student_model(features, training=True)\n",
        "    loss, log_probs_student = student_headl(pooled_output_student, labels, True)      \n",
        "  grads = g.gradient(loss, student_model.trainable_weights+student_headl.trainable_weights)\n",
        "  return loss, log_probs_student, grads\n",
        "\n",
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=True)\n",
        "dataset = train_input_fn({'batch_size': 32})\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "opt = tf.keras.optimizers.Adam(FLAGS.learning_rate)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "for i, ex in enumerate(tqdm(dataset.take(FLAGS.num_train_steps), position=0)):\n",
        "  loss, log_probs, grads = student_fwd_bwd(ex[0], ex[1])\n",
        "  opt.apply_gradients(zip(grads, student_model.trainable_weights+student_headl.trainable_weights))\n",
        "  train_loss(loss)\n",
        "  # print(train_loss(loss))\n",
        "  train_accuracy(tf.one_hot(ex[1], 2), log_probs)\n",
        "  if (i+1)% 50 == 0:\n",
        "    print('Loss = {}  Accuracy = {}'.format(train_loss.result().numpy(), train_accuracy.result().numpy()))\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def fwd_only(features, labels):\n",
        "  _, pooled_output = student_model(features, training=False)\n",
        "  loss, log_probs = student_headl(pooled_output, labels, False)\n",
        "  return loss, log_probs\n",
        "\n",
        "\n",
        "eval_input_fn = run_classifier.input_fn_builder(\n",
        "        data_dir=FLAGS.data_dir,\n",
        "        vocab_model_file=FLAGS.vocab_model_file,\n",
        "        max_encoder_length=FLAGS.max_encoder_length,\n",
        "        substitute_newline=FLAGS.substitute_newline,\n",
        "        is_training=False)\n",
        "eval_dataset = eval_input_fn({'batch_size': 32})\n",
        "\n",
        "eval_loss = tf.keras.metrics.Mean(name='eval_loss')\n",
        "eval_accuracy = tf.keras.metrics.CategoricalAccuracy(name='eval_accuracy')\n",
        "\n",
        "for ex in tqdm(eval_dataset, position=0):\n",
        "  loss, log_probs = fwd_only(ex[0], ex[1])\n",
        "  eval_loss(loss)\n",
        "  eval_accuracy(tf.one_hot(ex[1], 2), log_probs)\n",
        "print('Loss = {}  Accuracy = {}'.format(eval_loss.result().numpy(), eval_accuracy.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  5%|▌         | 50/1000 [01:06<15:43,  1.01it/s]Loss = 0.69440758228302  Accuracy = 0.4975000023841858\n",
            " 10%|█         | 100/1000 [01:56<14:54,  1.01it/s]Loss = 0.6949686408042908  Accuracy = 0.4987500011920929\n",
            " 15%|█▌        | 150/1000 [02:45<14:01,  1.01it/s]Loss = 0.6937657594680786  Accuracy = 0.5091666579246521\n",
            " 20%|██        | 200/1000 [03:35<13:12,  1.01it/s]Loss = 0.6920759081840515  Accuracy = 0.5221874713897705\n",
            " 25%|██▌       | 250/1000 [04:24<12:28,  1.00it/s]Loss = 0.6906613111495972  Accuracy = 0.5267500281333923\n",
            " 30%|███       | 300/1000 [05:14<11:35,  1.01it/s]Loss = 0.6866089105606079  Accuracy = 0.5407291650772095\n",
            " 35%|███▌      | 350/1000 [06:03<10:48,  1.00it/s]Loss = 0.6828024387359619  Accuracy = 0.5516071319580078\n",
            " 40%|████      | 400/1000 [06:53<09:57,  1.00it/s]Loss = 0.6743711829185486  Accuracy = 0.567578136920929\n",
            " 45%|████▌     | 450/1000 [07:43<09:03,  1.01it/s]Loss = 0.6637136340141296  Accuracy = 0.581458330154419\n",
            " 50%|█████     | 500/1000 [08:32<08:13,  1.01it/s]Loss = 0.6512491703033447  Accuracy = 0.5956875085830688\n",
            " 55%|█████▌    | 550/1000 [09:21<07:25,  1.01it/s]Loss = 0.6389033198356628  Accuracy = 0.6096590757369995\n",
            " 60%|██████    | 600/1000 [10:11<06:34,  1.01it/s]Loss = 0.6272562742233276  Accuracy = 0.6217708587646484\n",
            " 65%|██████▌   | 650/1000 [11:00<05:45,  1.01it/s]Loss = 0.614691972732544  Accuracy = 0.6338461637496948\n",
            " 70%|███████   | 700/1000 [11:49<04:57,  1.01it/s]Loss = 0.6039265990257263  Accuracy = 0.6443750262260437\n",
            " 75%|███████▌  | 750/1000 [12:39<04:07,  1.01it/s]Loss = 0.5932496190071106  Accuracy = 0.6534166932106018\n",
            " 80%|████████  | 800/1000 [13:28<03:18,  1.01it/s]Loss = 0.5813524723052979  Accuracy = 0.6637109518051147\n",
            " 85%|████████▌ | 850/1000 [14:18<02:28,  1.01it/s]Loss = 0.5708072185516357  Accuracy = 0.6727573275566101\n",
            " 90%|█████████ | 900/1000 [15:07<01:39,  1.01it/s]Loss = 0.5599746108055115  Accuracy = 0.6814236044883728\n",
            " 95%|█████████▌| 950/1000 [15:57<00:49,  1.02it/s]Loss = 0.5526861548423767  Accuracy = 0.687598705291748\n",
            "100%|██████████| 1000/1000 [16:46<00:00,  1.01s/it]\n",
            "Loss = 0.543519139289856  Accuracy = 0.6950312256813049\n",
            "--- 1006.4653191566467 seconds ---\n",
            "100%|██████████| 781/781 [03:11<00:00,  4.08it/s]Loss = 0.378199964761734  Accuracy = 0.703053507804871\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKj8rIqV8m0_",
        "outputId": "2f941adc-6817-4ac2-ba15-f8f6bd19e09d"
      },
      "source": [
        "print(\"student_model params:\",student_model.count_params())\n",
        "print(\"teacher_model params:\",model.count_params())\n",
        "(student_model.count_params()/model.count_params())*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "student_model params: 51873792\n",
            "teacher_model params: 127468800\n",
            "40.6952854345534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOxffxpIMMN5",
        "scrolled": true,
        "outputId": "e5ad9015-6e15-4fc3-b366-6550c8814327"
      },
      "source": [
        "2804.71/60"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "46.74516666666667"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MW_k9cHka0m"
      },
      "source": [
        "\n",
        "* Test set accuracy: IMDB dataset \n",
        "  * Full attention - 0.93305\n",
        "  * Sparse attention - 0.90625\n",
        "  * Sparse attention with KD (student model) - 0.87947\n",
        "  * Sparse attention without KD (student model) - 0.703053\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Number of parameters in student model : 51 million\n",
        "* Number of parameters in teacher model : 127 million\n",
        "* Size of student model is 40% of teacher model\n",
        "\n",
        "\n",
        "\n",
        "* Time taken to train original full attention: 132.4 mins\n",
        "* Time taken to train sparse attention: 115.5 mins\n",
        "* Time taken to train sparse attention with KD (student model): 46.74 mins\n",
        "* Time taken to train sparse attention without KD (student model): 16.77 mins\n",
        "\n",
        "\n",
        "* Sequence length - 512\n",
        "* Number of epochs - 1000\n",
        "* Batch size - 32\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49P0SlfjzK7c"
      },
      "source": [
        "# Teacher accuracy: 0.97687\n",
        "# Student accuracy: 0.9456\n",
        "# KT accuracy: 0.95370"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}