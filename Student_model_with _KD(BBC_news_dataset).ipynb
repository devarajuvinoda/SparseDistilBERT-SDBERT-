{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfrvAt8lO2JO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Standard Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Data Preprocessing & NLP\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import gensim\n",
        "from textblob import Word\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# Models\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.multiclass import OneVsRestClassifier\n",
        "# from sklearn.svm import SVC, LinearSVC\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingRegressor, GradientBoostingClassifier,BaggingClassifier\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Performance metrics\n",
        "# from sklearn.metrics import confusion_matrix, f1_score\n",
        "# from sklearn.model_selection import cross_val_score, KFold\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, make_scorer, roc_curve, roc_auc_score\n",
        "# from sklearn.metrics import precision_recall_fscore_support as score\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from PIL import Image\n",
        "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "# sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "from tensorflow.keras.utils import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from keras.layers import Flatten\n",
        "\n",
        "np.random.seed(101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z2XWwPPttzK",
        "outputId": "6e777360-8cdf-4c46-b17a-1d49867bf5d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CJFznnTNEW"
      },
      "source": [
        "# Data\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhwpeIrSaejc",
        "outputId": "c161ec87-b79f-4145-d133-df65f6779d85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sport            346\n",
              "business         336\n",
              "politics         274\n",
              "entertainment    273\n",
              "tech             261\n",
              "Name: Category, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news_df = pd.read_csv(\"/content/drive/MyDrive/Data/A4/TrainData.csv\")\n",
        "news_df['Category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Z4JJWfaeb1",
        "outputId": "411be85c-512a-4702-ed24-7dd637e84141"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
        "enc.fit(np.array(news_df[\"Category\"]).reshape(-1,1))\n",
        "y_train = enc.transform(np.array(news_df[\"Category\"]).reshape(-1,1)).toarray()\n",
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrtQxFCvaeT_",
        "outputId": "60a56a98-b854-4eb5-9cd0-6b1a7e169127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.55-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 64.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85453 sha256=81f0bf440876c8ef05831a89ba41fcc84e520b8a96c07f60befa77c9bafbf0ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.55 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPb6jl3kaeMF"
      },
      "outputs": [],
      "source": [
        "import contractions\n",
        "# Data Cleaning\n",
        "def clean_text(text):\n",
        "    clean_texts = \"\"\n",
        "    expanded_words = \"\"\n",
        "    # remove everything except alphabets\n",
        "    # text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "\n",
        "    words = text.split(\" \")\n",
        "\n",
        "    for word in words:\n",
        "      word = contractions.fix(word)\n",
        "      tokens = word.split(\" \")\n",
        "      for token in tokens:\n",
        "        if(len(token) > 1):\n",
        "          expanded_words = expanded_words + ' ' + token  \n",
        "    clean_texts = clean_texts + ' ' + expanded_words\n",
        "    # remove whitespaces\n",
        "    clean_texts = ' '.join(clean_texts.split())\n",
        "    clean_texts = clean_texts.lower()\n",
        "    \n",
        "    return clean_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_JF5fFzaeE0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# creating clean text feature\n",
        "news_df['clean_text'] = news_df['Text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBbZR-EyawVc"
      },
      "outputs": [],
      "source": [
        "X = news_df.loc[:,'clean_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SubfdommawP8",
        "outputId": "0c52fafc-6465-4ded-8892-886898b8b715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_seq_length =  3165\n",
            "avg_seq_length =  367.7281879194631\n",
            "worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against battery of fraud charges have called company whistleblower as their first witness. cynthia cooper worldcom ex-head of internal accounting alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy. prosecution lawyers have argued that mr ebbers orchestrated series of accounting tricks at worldcom ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper who now runs her own consulting business told jury in new york on wednesday that external auditors arthur andersen had approved worldcom accounting in early 2001 and 2002. she said andersen had given green light to the procedures and practices used by worldcom. mr ebber lawyers have said he was unaware of the fraud arguing that auditors did not alert him to any problems. ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company finance chief giving only brief answers himself. the prosecution star witness former worldcom financial chief scott sullivan has said that mr ebbers ordered accounting adjustments at the firm telling him to hit our books however ms cooper said mr sullivan had not mentioned anything uncomfortable about worldcom accounting during 2001 audit committee meeting. mr ebbers could face jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004 and is now known as mci. last week mci agreed to buyout by verizon communications in deal valued at $6.75bn.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = max([len(text.split(\" \")) for text in X])\n",
        "avg_seq_length = np.mean([len(text.split(\" \")) for text in X])\n",
        "print(\"max_seq_length = \" , max_seq_length)\n",
        "print(\"avg_seq_length = \" , avg_seq_length)\n",
        "print(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VLKub1TawKl",
        "outputId": "87607824-a154-4679-e89e-c1708d0552ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (1490, 1000)\n",
            "y_train shape: (1490, 5)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words =  1000 #3000\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = \"OOV\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "sequence_train = tokenizer.texts_to_sequences(X)\n",
        "sequence_train = pad_sequences(sequence_train,padding='post', maxlen=max_words)\n",
        "x_train = np.asarray(sequence_train)\n",
        "\n",
        "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nPL7IMRawFG",
        "outputId": "3ed67177-5b0e-4085-f6ec-e6bbae762a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_test shape: (735, 1000)\n",
            "y_test shape: (735, 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleId</th>\n",
              "      <th>Text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1018</td>\n",
              "      <td>qpr keeper day heads for preston queens park r...</td>\n",
              "      <td>qpr keeper day heads for preston queens park r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1319</td>\n",
              "      <td>software watching while you work software that...</td>\n",
              "      <td>software watching while you work software that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1138</td>\n",
              "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
              "      <td>arcy injury adds to ireland woe gordon arcy ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>459</td>\n",
              "      <td>india s reliance family feud heats up the ongo...</td>\n",
              "      <td>india reliance family feud heats up the ongoin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1020</td>\n",
              "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
              "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ArticleId  ...                                         clean_text\n",
              "0       1018  ...  qpr keeper day heads for preston queens park r...\n",
              "1       1319  ...  software watching while you work software that...\n",
              "2       1138  ...  arcy injury adds to ireland woe gordon arcy ha...\n",
              "3        459  ...  india reliance family feud heats up the ongoin...\n",
              "4       1020  ...  boro suffer morrison injury blow middlesbrough...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "news_test_df = pd.read_csv(\"/content/drive/MyDrive/Data/A4/TestData_Inputs.csv\")\n",
        "news_test_df2 = pd.read_excel(\"/content/drive/MyDrive/Data/A4/Assignment4_TestLabels.xlsx\")\n",
        "# creating clean text feature\n",
        "\n",
        "news_test_df['clean_text'] = news_test_df['Text'].apply(clean_text)\n",
        "# news_test_df['clean_text'] = news_test_df['clean_text'].apply(lambda x: remove_stopwords(x))\n",
        "X_test = news_test_df.loc[:,'clean_text']\n",
        "\n",
        "y_test = enc.transform(np.array(news_test_df2['Label - (business, tech, politics, sport, entertainment)']).reshape(-1,1)).toarray()\n",
        "\n",
        "sequence_test = tokenizer.texts_to_sequences(X_test)\n",
        "sequence_test = pad_sequences(sequence_test,padding='post', maxlen=max_words)\n",
        "X_test = np.asarray(sequence_test)\n",
        "\n",
        "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
        "print('x_test shape:', X_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "news_test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWyRTOTdav_W",
        "outputId": "80377290-da43-4e11-9acf-44a6afdb7350"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'OOV': 1,\n",
              " 'and': 5,\n",
              " 'for': 7,\n",
              " 'in': 6,\n",
              " 'is': 8,\n",
              " 'it': 10,\n",
              " 'of': 4,\n",
              " 'that': 9,\n",
              " 'the': 2,\n",
              " 'to': 3}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(list((tokenizer.word_index).items())[0:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T__VMpwCj47g"
      },
      "outputs": [],
      "source": [
        "# embedding_matrix_w2v = np.load(\"/content/drive/MyDrive/Data/A4/A4_matrix_w2v2.npy\")\n",
        "# embedding_matrix_ft = np.load(\"/content/drive/MyDrive/Data/A4/A4_matrix_ft2.npy\")\n",
        "# embedding_matrix_glove = np.load(\"/content/drive/MyDrive/Data/A4/A4_matrix_glove2.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFeQ5G-VkJlJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udEXAB52fx6p"
      },
      "source": [
        "# Positional Encoding\n",
        "A vector added to the embedding to encode positional information\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/transformer#positional_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_HhzsSNRFs8"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI-t0hWpRxGj"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKQrhS15MQQ"
      },
      "source": [
        "## Multi Headed Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QXu7IDrmGT7"
      },
      "source": [
        "Test of multi-headed Attention    \n",
        "Shape = (batch_size , num_heads, seq_length, depth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gk0dIJP6z35"
      },
      "source": [
        "For each of [query, value, key]   \n",
        "we reshape from  (batch_size , seq_length, depth) ->  (batch_size , seq_length, num_heads, multi_headed_depth)   \n",
        "then permute  ->  (batch_size , num_heads, seq_length, multi_headed_depth)  \n",
        "where multi-headed_depth = depth / num_heads\n",
        "\n",
        "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNZRelVEt3zr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model = 512, num_heads = 8, causal=False, dropout=0.0):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    assert d_model % num_heads == 0\n",
        "    depth = d_model // num_heads\n",
        "\n",
        "    self.w_query = tf.keras.layers.Dense(d_model)\n",
        "    self.split_reshape_query = tf.keras.layers.Reshape((-1,num_heads,depth))  \n",
        "    self.split_permute_query = tf.keras.layers.Permute((2,1,3))      \n",
        "\n",
        "    self.w_value = tf.keras.layers.Dense(d_model)\n",
        "    self.split_reshape_value = tf.keras.layers.Reshape((-1,num_heads,depth))\n",
        "    self.split_permute_value = tf.keras.layers.Permute((2,1,3))\n",
        "\n",
        "    self.w_key = tf.keras.layers.Dense(d_model)\n",
        "    self.split_reshape_key = tf.keras.layers.Reshape((-1,num_heads,depth))\n",
        "    self.split_permute_key = tf.keras.layers.Permute((2,1,3))\n",
        "\n",
        "    self.attention = tf.keras.layers.Attention(causal=causal, dropout=dropout)\n",
        "    self.join_permute_attention = tf.keras.layers.Permute((2,1,3))\n",
        "    self.join_reshape_attention = tf.keras.layers.Reshape((-1,d_model))\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    q = inputs[0]\n",
        "    v = inputs[1]\n",
        "    k = inputs[2] if len(inputs) > 2 else v\n",
        "\n",
        "    query = self.w_query(q)\n",
        "    query = self.split_reshape_query(query)    \n",
        "    query = self.split_permute_query(query)                 \n",
        "\n",
        "    value = self.w_value(v)\n",
        "    value = self.split_reshape_value(value)\n",
        "    value = self.split_permute_value(value)\n",
        "\n",
        "    key = self.w_key(k)\n",
        "    key = self.split_reshape_key(key)\n",
        "    key = self.split_permute_key(key)\n",
        "\n",
        "    if mask is not None:\n",
        "      if mask[0] is not None:\n",
        "        mask[0] = tf.keras.layers.Reshape((-1,1))(mask[0])\n",
        "        mask[0] = tf.keras.layers.Permute((2,1))(mask[0])\n",
        "      if mask[1] is not None:\n",
        "        mask[1] = tf.keras.layers.Reshape((-1,1))(mask[1])\n",
        "        mask[1] = tf.keras.layers.Permute((2,1))(mask[1])\n",
        "\n",
        "    attention = self.attention([query, value, key], mask=mask)\n",
        "    attention = self.join_permute_attention(attention)\n",
        "    attention = self.join_reshape_attention(attention)\n",
        "\n",
        "    x = self.dense(attention)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeGiHEQU8hlU"
      },
      "source": [
        "## Encoder Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7KMjvg6oeZF"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,  d_model = 512, num_heads = 8, dff = 2048, dropout = 0.0):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.multi_head_attention =  MultiHeadAttention(d_model, num_heads)\n",
        "    self.dropout_attention = tf.keras.layers.Dropout(dropout)\n",
        "    self.add_attention = tf.keras.layers.Add()\n",
        "    self.layer_norm_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.dense2 = tf.keras.layers.Dense(d_model)\n",
        "    self.dropout_dense = tf.keras.layers.Dropout(dropout)\n",
        "    self.add_dense = tf.keras.layers.Add()\n",
        "    self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    # print(mask)\n",
        "    attention = self.multi_head_attention([inputs,inputs,inputs], mask = [mask,mask])\n",
        "    attention = self.dropout_attention(attention, training = training)\n",
        "    x = self.add_attention([inputs , attention])\n",
        "    x = self.layer_norm_attention(x)\n",
        "    # x = inputs\n",
        "\n",
        "    ## Feed Forward\n",
        "    dense = self.dense1(x)\n",
        "    dense = self.dense2(dense)\n",
        "    dense = self.dropout_dense(dense, training = training)\n",
        "    x = self.add_dense([x , dense])\n",
        "    x = self.layer_norm_dense(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5hZJABBrvod"
      },
      "source": [
        "the causal = True argument for multi_head_attention1 automatically masks  future tokens in the sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRONqwb78use"
      },
      "source": [
        "## Encoder Blocks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zh-sfOMzLRQ"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, num_layers = 4, d_model = 512, num_heads = 8, dff = 2048, maximum_position_encoding = 10000, dropout = 0.0):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model, mask_zero=True)\n",
        "    self.pos = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.encoder_layers = [ EncoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    x = self.embedding(inputs)\n",
        "    # positional encoding\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # scaling by the sqrt of d_model, not sure why or if needed??\n",
        "    x += self.pos[: , :tf.shape(x)[1], :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    #Encoder layer\n",
        "    embedding_mask = self.embedding.compute_mask(inputs)\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, mask = embedding_mask)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return self.embedding.compute_mask(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOsWp_L19qm8"
      },
      "source": [
        "# Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wOZpgGc8hSH",
        "outputId": "30aaea86-cba6-4f22-8fd7-c9da0f302ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 15s 299ms/step\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 1000)]            0         \n",
            "_________________________________________________________________\n",
            "encoder_1 (Encoder)          (None, 1000, 64)          1631488   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               49920     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 1,682,053\n",
            "Trainable params: 1,682,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_layers = 1\n",
        "d_model = 64\n",
        "dff = 128\n",
        "num_heads = 4\n",
        "dropout_rate = 0.4\n",
        "MAX_LENGTH = max_words\n",
        "alpha = 0.2\n",
        "\n",
        "input_vocab_size = len(tokenizer.index_word) + 1\n",
        "\n",
        "input = tf.keras.layers.Input(shape=(MAX_LENGTH,))\n",
        "encoder = Encoder(input_vocab_size, num_layers = num_layers, d_model = d_model, num_heads = num_heads,\n",
        "                  dff = dff, dropout = dropout_rate)\n",
        "\n",
        "x = encoder(input)\n",
        "x = Bidirectional(GRU(64, dropout=0.2))(x)\n",
        "output = Dense(5, activation='sigmoid')(x)\n",
        "# flatten = Flatten()(x)\n",
        "# dense1 = Dense(256, activation=\"relu\")(flatten)\n",
        "# dropout = Dropout(0.2)(dense1)\n",
        "# output = Dense(5, activation=\"sigmoid\")(dropout)\n",
        "  \n",
        "model = Model(input, output)\n",
        "\n",
        "teacher_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Data/Teacher_model\")\n",
        "teacher_y_pred = teacher_model.predict(x_train, verbose=1)\n",
        "T_y_pred = K.constant(teacher_y_pred)\n",
        "\n",
        "def custom_loss_function(y_true, y_pred):\n",
        "  student_loss = - y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
        "\n",
        "  distil_loss = - T_y_pred * tf.math.log(y_pred) - (1 - T_y_pred) * tf.math.log(1 - y_pred)\n",
        "\n",
        "  loss = alpha * student_loss + (1-alpha) * distil_loss\n",
        "  print(tf.reduce_mean(loss, axis=-1))\n",
        "  return tf.reduce_mean(loss, axis=-1)\n",
        "\n",
        "model.compile(loss = custom_loss_function, optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv_43SkD5wlN",
        "outputId": "ad24a5aa-da08-45ce-f66a-a1b79955af4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1490, 5)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(teacher_y_pred.shape)\n",
        "type(teacher_y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9BrnE6_m4Id"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0adrYXdKtZOG",
        "outputId": "4e200520-40de-4b93-9442-b75a89358206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "Tensor(\"custom_loss_function/Mean:0\", shape=(None,), dtype=float32)\n",
            "Tensor(\"custom_loss_function/Mean:0\", shape=(None,), dtype=float32)\n",
            "Tensor(\"custom_loss_function/Mean:0\", shape=(None,), dtype=float32)\n",
            "75/75 - 34s - loss: 0.5123 - accuracy: 0.2525 - val_loss: 0.5120 - val_accuracy: 0.2517\n",
            "Epoch 2/15\n",
            "75/75 - 19s - loss: 0.4531 - accuracy: 0.4111 - val_loss: 0.4389 - val_accuracy: 0.4664\n",
            "Epoch 3/15\n",
            "75/75 - 19s - loss: 0.3631 - accuracy: 0.5990 - val_loss: 0.3091 - val_accuracy: 0.6577\n",
            "Epoch 4/15\n",
            "75/75 - 19s - loss: 0.2121 - accuracy: 0.8331 - val_loss: 0.1383 - val_accuracy: 0.8859\n",
            "Epoch 5/15\n",
            "75/75 - 19s - loss: 0.0938 - accuracy: 0.9455 - val_loss: 0.0809 - val_accuracy: 0.9362\n",
            "Epoch 6/15\n",
            "75/75 - 19s - loss: 0.0457 - accuracy: 0.9757 - val_loss: 0.0789 - val_accuracy: 0.9362\n",
            "Epoch 7/15\n",
            "75/75 - 19s - loss: 0.0271 - accuracy: 0.9807 - val_loss: 0.0807 - val_accuracy: 0.9430\n",
            "Epoch 8/15\n",
            "75/75 - 19s - loss: 0.0145 - accuracy: 0.9924 - val_loss: 0.0742 - val_accuracy: 0.9497\n",
            "Epoch 9/15\n",
            "75/75 - 19s - loss: 0.0080 - accuracy: 0.9975 - val_loss: 0.0769 - val_accuracy: 0.9430\n",
            "Epoch 10/15\n",
            "75/75 - 19s - loss: 0.0062 - accuracy: 0.9975 - val_loss: 0.0867 - val_accuracy: 0.9396\n",
            "Epoch 11/15\n",
            "75/75 - 19s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0796 - val_accuracy: 0.9497\n",
            "Epoch 12/15\n",
            "75/75 - 19s - loss: 0.0061 - accuracy: 0.9975 - val_loss: 0.1118 - val_accuracy: 0.9329\n",
            "Epoch 13/15\n",
            "75/75 - 19s - loss: 0.0100 - accuracy: 0.9933 - val_loss: 0.1042 - val_accuracy: 0.9228\n",
            "Epoch 14/15\n",
            "75/75 - 19s - loss: 0.0042 - accuracy: 0.9983 - val_loss: 0.0744 - val_accuracy: 0.9564\n"
          ]
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode = 'min' ,patience=6)\n",
        "history = model.fit(x_train , y_train,\n",
        "                    batch_size=16,\n",
        "                    epochs=15,\n",
        "                    verbose=2,\n",
        "                    callbacks=[callback],\n",
        "                    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6fwqq9ZNOfk",
        "outputId": "a1058a60-5f2a-4e0c-bd27-e1c62a24dbe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 3s 143ms/step - loss: 0.0708 - accuracy: 0.9537\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ADL_proj2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
